# -*- coding: utf-8 -*-
"""cc-xp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17zK-thdBcoDMG4tIl0WKscgZ-chLPUuJ
"""

! pip install scikit-plot

# imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_samples, silhouette_score

import scikitplot as skplot

cc_org = pd.read_csv("CC GENERAL.csv")
cc_org.shape

cc_org.head()

cc_org.info()

# find out there is missing value under "minimum payments" and "cridit_limit"
cc_org.isna().sum()

cc_org.isna().mean()*100

# drop "cust_id" since it doesn't have any role in determining the cluster
cc_clean = cc_clean.drop(columns = "CUST_ID")

# Just 1 value is missing at CREDIT_LIMIT feature, we can simply drop it and never think about it again.
cc_org.dropna(subset=["CREDIT_LIMIT"],inplace=True)

# change all missing value under "minimum payments" to median
cc_org['MINIMUM_PAYMENTS'].fillna(cc_org['MINIMUM_PAYMENTS'].median(), inplace=True)

cc_new = cc_clean.sample(n=1500)

# correlations
cc_corr = cc_new.corr()
sns.heatmap(cc_corr)

# using standardscaler to reduce the distance of each variable
sc = StandardScaler()
xs = sc.fit_transform(cc_new)
X = pd.DataFrame(xs, index=cc_new.index, columns=cc_new.columns)

# hclust
# going to do euclidean and cosine distance
diste = pdist(X.values)
distc = pdist(X.values, metric='cosine')
distj = pdist(X.values, metric='jaccard')
distm = pdist(X.values, metric ='cityblock')

hclust_e = linkage(diste)
hclust_c = linkage(distc)
hclust_m = linkage(distm)

#both plot
LINKS = [hclust_e, hclust_c, hclust_m]
TITLE = ['Euclidean','Cosine','Manhattan']

plt.figure(figsize=(15,5))

#loop and build our plot
for i, m in enumerate(LINKS):
  plt.subplot(1,3,i+1)
  plt.title(TITLE[i])
  dendrogram(m,
             #labels = ps.index
             leaf_rotation=90,
             # leaf_font_size=10,
             orientation='left')
plt.show()
# pick cosine

METHODS = ['single', 'complete', 'average','ward']
plt.figure(figsize=(20,5))

for i, m in enumerate(METHODS):
  plt.subplot(1,4,i+1)
  plt.title(m)
  dendrogram(linkage(distc, method=m),
             leaf_rotation=90)
plt.show()

plt.figure(figsize=(10, 6))

avg = linkage(X.values, method="average")
dendrogram(avg,
          labels = X.index,
          leaf_rotation=90,
          leaf_font_size=10, color_threshold=4)

plt.axhline(y=7)
plt.show()

labs = fcluster(linkage(distc, method='average'), 7, criterion='maxclust')

np.unique(labs)

cc_new['cluster'] = labs

cc_new.head()

#How many credit card per cluster assignment?
cc_new.cluster.value_counts(dropna=False,sort=False)

















